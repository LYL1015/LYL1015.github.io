---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<div class="intro-animation">
Welcome to my academic homepage. I am Yunlong Lin, a Master student at Xiamen University (XMU) @[SmartDSP](https://xmu-smartdsp.github.io/) advised by [Prof. Xinghao Ding](https://scholar.google.com.hk/citations?user=k5hVBfMAAAAJ&hl=zh-CN). My research focuses on advancing computational photography for visual enhancement and scene reconstruction, developing multimodal learning frameworks for cross-modal understanding, contextual reasoning, and adaptive skill acquisition, and building intelligent agents capable of perception, decision-making, and autonomous action. Key research areas and methodologies include:

<div class="research-areas">
  <div class="research-item" data-tooltip="Vision and language understanding, multimodal reasoning">
    <div class="research-icon">üß†</div>
    <div class="research-content">
      <h3><span style="color: #FFB6C1">Multimodal learning</span></h3>
      <p>Vision and language, Visual reasoning, Generalist models</p>
    </div>
  </div>
  
  <div class="research-item" data-tooltip="Image restoration and enhancement, generative priors">
    <div class="research-icon">üîç</div>
    <div class="research-content">
      <h3><span style="color: #008B8B">Inverse Problems</span></h3>
      <p>Real-world degradation restoration, Generative priors</p>
    </div>
  </div>
  
  <div class="research-item" data-tooltip="Agent planning and decision-making, reinforcement learning">
    <div class="research-icon">ü§ñ</div>
    <div class="research-content">
      <h3><span style="color: #87CEEB">AI Agents</span></h3>
      <p>Planning and Decision-Making, Reinforcement learning</p>
    </div>
  </div>
  
  <div class="research-item" data-tooltip="3D environment perception, spatial reasoning">
    <div class="research-icon">üåê</div>
    <div class="research-content">
      <h3><span style="color:rgb(226, 96, 10)">Spatial Intelligence</span></h3>
      <p>3D Environment Perception, LLM-based Spatial Reasoning, Agent-driven Decision-making and Action</p>
    </div>
  </div>
</div>

<!-- - <span style="color:rgb(232, 96, 96)">**AI Agents**</span>: Planning and Decision-Making, Reinforcement learning -->

I am actively seeking collaborations and currently looking for <span style="color:rgb(232, 96, 96)">**26 fall PhD positions**</span>! If you are interested in working together or have potential PhD opportunities, please feel free to reach out to me. I am eager to join teams or labs that value innovation in AI-driven perception, cross-modal learning, and AI agent systems. 
</div>

<div id="motto" class="typed-motto"></div>

# üì± Contact
- **WeChat**: lyl20136148
- **Email**: linyl@stu.xmu.edu.cn



# üî• News
<div class="news-container">
- Three papers accepted by CVPR'25!
- Four papers accepted by AAAI'25 (2 oral)!
- One paper accepted by ECCV'24!
- Two paper accepted by TGRS'25!
- Two paper accepted by TCSVT'24!
- Two paper accepted by TGRS'24!
- Three papers accepted by ACMMM'23 (1 oral)!
</div>

# üìù Publications
<!-- <p style='text-align: justify;'> My current research focuses on three main areas: <strong>(I)</strong> Addressing the challenges of real-world image restoration and enhancement by identifying and overcoming the limitations of existing learning-based methods. <strong>(II)</strong> Exploring vision problems related to photography, with the goal of producing images of superior visual quality. <strong>(III)</strong> Providing support for the art creation industry and digital asset generation through advancements in AIGC (Artificial Intelligence Generated Content) technology.
</p> -->

<!-- ------------------------------------ -->

<!-- Paper 1 -->
<div class='paper-box paper-animation'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><video src='images/papers/jarvisir.mp4' alt="sym" width="100%" autoplay loop muted></video></div></div>
<div class='paper-box-text' markdown="1">

[**JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration**]()

**Yunlong Lin\***, Zixu Lin\*, Haoyu Chen\*, Panwang Pan\*, Chenxin Li, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, Xinghao Ding

[<a href="./papers/CVPR2025_JarvisIR.pdf">PDF</a>] | [Project](https://cvpr2025-jarvisir.github.io/) | [Code](https://github.com/LYL1015/JarvisIR)
</div>
</div>

<!-- Paper 1 -->
<div class='paper-box paper-animation'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><video src='images/papers/LIE.mp4' alt="sym" width="100%" autoplay loop muted></video></div></div>
<div class='paper-box-text' markdown="1">

[**AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement**](https://arxiv.org/pdf/2407.14900)

**Yunlong Lin\***, Tian Ye\*, Sixiang Chen\*, Zhenqi Fu, Yingying Wang, Wenhao Chai, Zhaohu Xing, Lei Zhu, Xinghao Ding.

[PDF](https://arxiv.org/pdf/2407.14900) | [Project](https://aglldiff.github.io) | [Code](https://github.com/LYL1015/AGLLDiff)
</div>
</div>

<!-- ------------------------------------ -->
<!-- Paper 2 -->
<div class='paper-box paper-animation'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/papers/dplut.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors**](https://arxiv.org/pdf/2409.18899)

**Yunlong Lin\***, Zhenqi Fu\*, Kairun Wen, Tian Ye, Sixiang Chen, Ge Meng, Yingying Wang, Yue Huang, Xiaotong Tu, Xinghao Ding.

[PDF](https://arxiv.org/pdf/2409.18899) | [Project](https://dplut.github.io/)
</div>
</div>


<!-- Paper 2 -->
<div class='paper-box paper-animation'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/papers/snowmaster.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**SnowMaster: Comprehensive Real-world Image Desnowing via MLLM with Multi-Model Feedback Optimization**]()

Jianyu Lai\*, Sixiang Chen\*, **Yunlong Lin**, Tian Ye, Yun Liu, Song Fei, Zhaohu Xing, Hongtao Wu, Weiming Wang, Lei Zhu.

[PDF]() | [Project]()
</div>
</div>

<!-- # üìù Other Publications  -->
<ul class="publications-list">

  <!-- Paper 1 -->

  <!-- ------------------------------------ -->


  <!-- Paper 1 -->
  <li class="publication-item">
    <strong>Track Any Anomalous Object:A Granular Video Anomaly Detection Pipeline. CVPR 2025.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Yuzhi Huang, Chenxin Li, Haitao Zhang, Zixu Lin, <strong>Yunlong Lin</strong>, Hengyu Liu, Wuyang Li, Xinyu Liu, Jiechao Gao, Yue Huang, Xinghao Ding, Yixuan Yuan.</i></div>
  </li>

  <!-- ------------------------------------ -->

  
  <!-- Paper 1 -->
  <!-- <li>
    <strong>Sp3ctralMamba: Physics-Driven Joint State Space Model for Hyperspectral Image Reconstruction. AAAI 2025 Oral.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Ge Meng, Jingyan Tu, Jingjia Huang, <strong>Yunlong Lin</strong>, Yingying Wang, Xiaotong Tu, Yue Huang, Xinghao Ding</i></div>
  </li> -->

  <!-- ------------------------------------ -->

  <!-- Paper 1 -->
  <!-- <li>
    <strong>Accelerated Diffusion via High-Low Frequency Decomposition for Pan-sharpening. AAAI 2025 Oral.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Ge Meng, Jingjia Huang, Jingyan Tu, Yingying Wang, <strong>Yunlong Lin</strong>, Xiaotong Tu, Yue Huang, Xinghao Ding</i></div>
  </li> -->

  <!-- ------------------------------------ -->


  <!-- Paper 1 -->
  <li class="publication-item">
    <strong>Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint. ECCV 2024.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Sixiang Chen, Tian Ye, Kai Zhang, Zhaohu Xing, <strong>Yunlong Lin</strong>, Lei Zhu</i></div>
  </li>

  <!-- ------------------------------------ -->
  <!-- Paper 2 -->
  <li class="publication-item">
    <strong>Fusion2Void: Unsupervised Multi-focus Image Fusion Based on Image Inpainting. TCSVT 2024.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Huangxing Lin, <strong>Yunlong Lin</strong>, Jingyuan Xia, Linyu Fan, Feifei Li, Yingying Wang, Xinghao Ding</i></div>
  </li>

  <!-- ------------------------------------ -->
  <!-- Paper 3 -->
  <!-- <li>
    <strong>Frequency decoupled domain-irrelevant feature learning for Pan-sharpening. TCSVT 2024.</strong>
    <div style="display: inline">
        <a href="https://ieeexplore.ieee.org/abstract/document/10718360"> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Jie Zhang, Ke Cao, Keyu Yan, <strong>Yunlong Lin</strong>, Xuanhua He, Yingying Wang, Rui Li, Chengjun Xie, Jun Zhang, Man Zhou</i></div>
  </li> -->

  <!-- ------------------------------------ -->
  <!-- Paper 4 -->
  <!-- <li>
    <strong>Cross-Modality Interaction Network for Pan-sharpening. TGRS 2024.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Yingying Wang, Xuanhua He, Yuhang Dong, <strong>Yunlong Lin</strong>, Yue Huang, Xinghao Ding</i></div>
  </li> -->

  <!-- ------------------------------------ -->
  <!-- Paper 5 -->
  <li class="publication-item">
    <strong>Domain-irrelevant Feature Learning for Generalizable Pan-sharpening. ACMMM 2023.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i><strong>Yunlong Lin</strong>, Zhenqi Fu, Ge Meng, Yingying Wang, Yuhang Dong, Linyu Fan, Hedeng Yu, Xinghao Ding</i></div>
  </li>

  <!-- ------------------------------------ -->
  <!-- Paper 6 -->
  <!-- <li>
    <strong>Learning High-frequency Feature Enhancement and Alignment for Pan-sharpening. ACMMM 2023 Oral.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Yingying Wang, <strong>Yunlong Lin</strong>, Ge Meng, Zhenqi Fu, Yuhang Dong, Linyu Fan, Hedeng Yu, Xinghao Ding, Yue Huang</i></div>
  </li> -->

  <!-- ------------------------------------ -->
  <!-- Paper 7 -->
  <!-- <li>
    <strong>CPLFormer: Cross-scale Prototype Learning Transformer for Image Snow Removal. ACMMM 2023.</strong>
    <div style="display: inline">
        <a href=""> [paper]</a>
        <a href=""> [code]</a>
    </div>
    <div><i>Sixiang Chen, Tian Ye, Yun Liu, Jinbin Bai, Haoyu Chen, <strong>Yunlong Lin</strong>, Jun Shi, Erkang Chen</i></div>
  </li> -->
</ul>


# üéñ Honors and Awards
<div class="awards-container">
- NTIRE 2025 challenge on day and night raindrop removal for dual-focused images, third place.
- NTIRE 2025 Low Light Image Enhancement Challenge, sixth place.
- National Scholarship in Xiamen University, 2024
- Outstanding Graduate in Jimei University, 2023
- Second Price of Mathematical Contest In Modeling, 2021
- First Price of Mathorcup Mathematical Contest in Modeling, 2021
- First Price of Mathorcup Mathematical Contest in Modeling, 2022
</div>

# üìñ Educations
<div class="education-container">
- Sep'2023-Jul'2026: Master Student, Xiamen University
- Sep'2019-Jul'2023: B.Eng (Telecommunication Engineering), Jimei University, Xiamen
</div>

# üí¨ Academic Service
<div class="service-container">
- Conference Reviewer: ACMMM 2024/2025, NeurIPS 2024/2025, ICLR 2025, CVPR 2025, ICCV 2025, ICML 2025.
</div>

<!-- Ê∑ªÂä†Âä®ÁîªËÉåÊôØ -->
<div id="hexagon-background"></div>

<script>
document.addEventListener('DOMContentLoaded', function() {
  if (typeof Typed !== 'undefined') {
    new Typed('#motto', {
      strings: ["Exploring multimodal learning frontiers", "Solving inverse problems with generative priors", "Building agents with perception and decision capabilities", "Advancing spatial intelligence and 3D understanding"],
      typeSpeed: 50,
      backSpeed: 30,
      loop: true,
      startDelay: 1000,
      showCursor: true,
      cursorChar: '|'
    });
  } else {
    var script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/typed.js@2.0.12';
    script.onload = function() {
      new Typed('#motto', {
        strings: ["Exploring multimodal learning frontiers", "Solving inverse problems with generative priors", "Building agents with perception and decision capabilities", "Advancing spatial intelligence and 3D understanding"],
        typeSpeed: 50,
        backSpeed: 30,
        loop: true,
        startDelay: 1000,
        showCursor: true,
        cursorChar: '|'
      });
    };
    document.head.appendChild(script);
  }
  // ÂàõÂª∫ÂÖ≠ËæπÂΩ¢ËÉåÊôØ
  createHexagonBackground();
});

function createHexagonBackground() {
  const container = document.getElementById('hexagon-background');
  const hexCount = 20; // ÂÖ≠ËæπÂΩ¢Êï∞Èáè
  
  for (let i = 0; i < hexCount; i++) {
    const hexagon = document.createElement('div');
    hexagon.className = 'hexagon';
    
    // ÈöèÊú∫‰ΩçÁΩÆ
    const x = Math.random() * 100;
    const y = Math.random() * 100;
    
    // ÈöèÊú∫Â§ßÂ∞è
    const size = Math.random() * 80 + 40;
    
    // ÈöèÊú∫ÈÄèÊòéÂ∫¶
    const opacity = Math.random() * 0.15 + 0.02;
    
    // ÈöèÊú∫ÊóãËΩ¨
    const rotation = Math.random() * 360;
    
    // ÈöèÊú∫Âä®ÁîªÂª∂Ëøü
    const delay = Math.random() * 5;
    
    hexagon.style.cssText = `
      left: ${x}%;
      top: ${y}%;
      width: ${size}px;
      height: ${size * 0.866}px;
      opacity: ${opacity};
      transform: rotate(${rotation}deg);
      animation-delay: ${delay}s;
    `;
    
    container.appendChild(hexagon);
  }
}
</script>

<style>
/* Âü∫Êú¨Âä®ÁîªÊïàÊûú */
@keyframes fadeInUp {
  from {
    opacity: 0;
    transform: translateY(30px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* È°µÈù¢Êï¥‰ΩìÂÖÉÁ¥†Âä®Áîª */
.intro-animation,
.news-container,
.awards-container,
.education-container,
.service-container,
h1, h2, h3 {
  animation: fadeInUp 0.8s ease-out forwards;
  opacity: 0;
}

/* ËÆæÁΩÆ‰∏çÂêåÂª∂Ëøü */
h1 { animation-delay: 0.1s; }
.intro-animation { animation-delay: 0.3s; }
.news-container { animation-delay: 0.5s; }
.awards-container { animation-delay: 0.6s; }
.education-container { animation-delay: 0.7s; }
.service-container { animation-delay: 0.8s; }

/* ËÆ∫ÊñáÈ°πÁõÆÂä®Áîª */
.paper-animation {
  opacity: 0;
  animation: fadeInUp 0.8s ease-out forwards;
  animation-delay: 0.9s;
}

/* Á†îÁ©∂È¢ÜÂüüÊ†∑Âºè */
.research-areas {
  margin: 20px 0;
  padding: 0;
}

.research-item {
  background: rgba(255, 255, 255, 0.8);
  padding: 20px;
  margin: 15px 0;
  border-radius: 10px;
  box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  display: flex;
  align-items: flex-start;
  transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
  position: relative;
  overflow: hidden;
  backdrop-filter: blur(5px);
}

.research-item:hover {
  transform: translateY(-5px);
  box-shadow: 0 8px 16px rgba(0,0,0,0.2);
}

/* ÂõæÊ†áÊ†∑Âºè */
.research-icon {
  font-size: 28px;
  margin-right: 15px;
  transition: transform 0.5s cubic-bezier(0.4, 0, 0.2, 1);
}

.research-item:hover .research-icon {
  transform: rotate(15deg) scale(1.2);
}

/* ÂÜÖÂÆπÊ†∑Âºè */
.research-content {
  flex: 1;
}

.research-content h3 {
  margin: 0 0 10px 0;
  font-size: 1.2em;
  transition: transform 0.3s ease;
}

.research-content p {
  margin: 0;
  color: #666;
  transition: color 0.3s ease;
}

.research-item:hover .research-content p {
  color: #333;
}

/* ÊèêÁ§∫Â∑•ÂÖ∑Ê†∑Âºè */
.research-item {
  position: relative;
}

.research-item::after {
  content: attr(data-tooltip);
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  padding: 8px 12px;
  background-color: rgba(0, 0, 0, 0.8);
  color: white;
  border-radius: 6px;
  font-size: 14px;
  white-space: nowrap;
  opacity: 0;
  visibility: hidden;
  transition: all 0.3s ease;
  z-index: 1000;
}

.research-item:hover::after {
  opacity: 1;
  visibility: visible;
  bottom: calc(100% + 10px);
}

/* ÁÆ≠Â§¥Ê†∑Âºè */
.research-item::before {
  content: '';
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  border: 6px solid transparent;
  border-top-color: rgba(0, 0, 0, 0.8);
  opacity: 0;
  visibility: hidden;
  transition: all 0.3s ease;
}

.research-item:hover::before {
  opacity: 1;
  visibility: visible;
  bottom: calc(100% + 4px);
}

/* ÊâìÂ≠óÊú∫ÊïàÊûúÊ†∑Âºè */
.typed-motto {
  font-style: italic;
  font-size: 24px;
  margin: 20px 0;
  min-height: 36px;
  background: -webkit-linear-gradient(left, #1772d0, #6c5ce7);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-family: 'Georgia', serif;
  font-weight: 500;
  text-align: center;
  text-shadow: 1px 1px 2px rgba(0,0,0,0.1);
}

.typed-cursor {
  color: #1772d0;
  font-weight: bold;
}

/* ËÆ∫ÊñáÂàóË°®Âä®Áîª */
.publications-list .publication-item {
  opacity: 0;
  animation: fadeInUp 0.5s ease-out forwards;
}

.publications-list .publication-item:nth-child(1) { animation-delay: 1.0s; }
.publications-list .publication-item:nth-child(2) { animation-delay: 1.1s; }
.publications-list .publication-item:nth-child(3) { animation-delay: 1.2s; }

/* ÂÖ≠ËæπÂΩ¢ËÉåÊôØ */
#hexagon-background {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  z-index: -1;
  pointer-events: none;
  overflow: hidden;
}

.hexagon {
  position: absolute;
  background: linear-gradient(120deg, #1772d0, #6c5ce7);
  clip-path: polygon(50% 0%, 100% 25%, 100% 75%, 50% 100%, 0% 75%, 0% 25%);
  animation: float 15s infinite ease-in-out;
}

@keyframes float {
  0%, 100% {
    transform: translateY(0) rotate(0deg);
  }
  25% {
    transform: translateY(-20px) rotate(5deg);
  }
  50% {
    transform: translateY(0) rotate(0deg);
  }
  75% {
    transform: translateY(20px) rotate(-5deg);
  }
}

/* ÂìçÂ∫îÂºèË∞ÉÊï¥ */
@media (max-width: 768px) {
  .research-item {
    flex-direction: column;
  }
  
  .research-icon {
    margin-bottom: 10px;
  }
  
  .typed-motto {
    font-size: 18px;
  }
}
</style>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=jacK9ggqHSefN4z3yvCMPbr34roVzQhT1qc6eb2yeTA&cl=ffffff&w=a"></script>
